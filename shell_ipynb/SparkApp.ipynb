{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import django\n",
    "django.setup()\n",
    "from nagg.models import NewsItemCollection, NewsItem, NewsItemCollectionMembership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from brains import spark_common\n",
    "from pyspark import SparkContext, SparkConf, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT \"nagg_newsitemcollectionmembership\".\"id\", \"nagg_newsitemcollectionmembership\".\"newsitem_id\",\n",
    "       \"nagg_newsitemcollectionmembership\".\"newsitemcollection_id\", \"nagg_newsitemcollectionmembership\".\"data\"::text,\n",
    "       \"nagg_newsitem\".\"source\", \"nagg_newsitem\".\"url\", \"nagg_newsitem\".\"text\",\n",
    "       \"nagg_newsitem\".\"publish_date\", \"nagg_newsitem\".\"retrieval_date\", \"nagg_newsitem\".\"search_index\"::text,\n",
    "       \"nagg_newsitemcollection\".\"name\", \"nagg_newsitemcollection\".\"metadata\"::text\n",
    "FROM \"nagg_newsitemcollectionmembership\"\n",
    "  INNER JOIN \"nagg_newsitemcollection\"\n",
    "  ON (\"nagg_newsitemcollectionmembership\".\"newsitemcollection_id\" = \"nagg_newsitemcollection\".\"id\")\n",
    "  INNER JOIN \"nagg_newsitem\"\n",
    "  ON (\"nagg_newsitemcollectionmembership\".\"newsitem_id\" = \"nagg_newsitem\".\"id\")\n",
    "    WHERE \"nagg_newsitemcollectionmembership\".\"newsitemcollection_id\" = 2\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with spark_common.SparkAPP('ipynb spartapp exp') as app:\n",
    "    #nic = NewsItemCollection.objects.get(id=2)\n",
    "    #qs = NewsItemCollection.objects.filter(newsitemcollectionmembership__newsitemcollection_id=2)\n",
    "    #qs = qs.newsitemcollectionmembership_set.select_related()\n",
    "    #qs.query.add_fields(['text', 'source'], True)\n",
    "    #qs.extra(select={'membership_id': '\"nagg_newsitemcollectionmembership\".\"id\"'})\n",
    "    #df = app.data_from_from_sql(qs[:1000])\n",
    "\n",
    "    df = app.data_from_from_sql(sql)\n",
    "    df.show()\n",
    "    print(df.count())\n",
    "    rdd = df.map(lambda x: (x.id, guess_lang(x.text)[0][0]))\n",
    "    print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "Message: 'Command to send: m\\nd\\no6\\ne\\n'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 474, in send_command\n",
      "    logger.debug(\"Answer received: {0}\".format(answer))\n",
      "Message: 'Answer received: yv'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "Message: 'Command to send: m\\nd\\no7\\ne\\n'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 474, in send_command\n",
      "    logger.debug(\"Answer received: {0}\".format(answer))\n",
      "Message: 'Answer received: yv'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "Message: 'Command to send: m\\nd\\no8\\ne\\n'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 474, in send_command\n",
      "    logger.debug(\"Answer received: {0}\".format(answer))\n",
      "Message: 'Answer received: yv'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "Message: 'Command to send: m\\nd\\no9\\ne\\n'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 474, in send_command\n",
      "    logger.debug(\"Answer received: {0}\".format(answer))\n",
      "Message: 'Answer received: yv'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "Message: 'Command to send: m\\nd\\no10\\ne\\n'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 474, in send_command\n",
      "    logger.debug(\"Answer received: {0}\".format(answer))\n",
      "Message: 'Answer received: yv'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "Message: 'Command to send: m\\nd\\no12\\ne\\n'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 474, in send_command\n",
      "    logger.debug(\"Answer received: {0}\".format(answer))\n",
      "Message: 'Answer received: yv'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "Message: 'Command to send: m\\nd\\no13\\ne\\n'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 474, in send_command\n",
      "    logger.debug(\"Answer received: {0}\".format(answer))\n",
      "Message: 'Answer received: yv'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "Message: 'Command to send: m\\nd\\no15\\ne\\n'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 474, in send_command\n",
      "    logger.debug(\"Answer received: {0}\".format(answer))\n",
      "Message: 'Answer received: yv'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "Message: 'Command to send: m\\nd\\no16\\ne\\n'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 474, in send_command\n",
      "    logger.debug(\"Answer received: {0}\".format(answer))\n",
      "Message: 'Answer received: yv'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "Message: 'Command to send: m\\nd\\no17\\ne\\n'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='/dream/codes/nagg/debug.log'>\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 403, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/daniel/.virtualenvs/naggenv/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-3c909ad206ca>\", line 3, in <module>\n",
      "    df = app.data_from_from_sql(sql)\n",
      "  File \"/dream/codes/nagg/brains/spark_common.py\", line 86, in data_from_from_sql\n",
      "    df = sql_context.read.format('jdbc').options(\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 632, in read\n",
      "    return DataFrameReader(self)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\", line 49, in __init__\n",
      "    self._jreader = sqlContext._ssql_ctx.read()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\", line 111, in _ssql_ctx\n",
      "    self._scala_SQLContext = self._jvm.SQLContext(self._jsc.sc())\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 536, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 470, in send_command\n",
      "    logger.debug(\"Command to send: {0}\".format(command))\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1262, in debug\n",
      "    self._log(DEBUG, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1409, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1419, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1481, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 853, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 1041, in emit\n",
      "    StreamHandler.emit(self, record)\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 982, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.4/logging/__init__.py\", line 962, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 569, in <lambda>\n",
      "    id=self._target_id: _garbage_collect_object(cc, id))\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 241, in _garbage_collect_object\n",
      "    '\\ne\\n'\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 364, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/daniel/spark-1.5.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\", line 474, in send_command\n",
      "    logger.debug(\"Answer received: {0}\".format(answer))\n",
      "Message: 'Answer received: yv'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "#del app\n",
    "app = spark_common.SparkAPP('save exp')\n",
    "df = app.data_from_from_sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = df.map(lambda x: (x.id, guess_lang(x.text)[0][0]))\n",
    "rdd = rdd.repartition(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, 'partition 0 OK'), (True, 'partition 1 OK'), (True, 'partition 2 OK')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def worker_update_results(idx, iterator):\n",
    "    \"\"\"Updates data on the DB, each worker in their own\n",
    "    transaction.\n",
    "    \n",
    "    If we require consitency over the full result set, we \n",
    "    need to employ a two phase commit.\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(database=\"naggdb\", user=\"nagg\", password=\"nagg\", host=\"10.0.3.7\")\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.executemany(\"\"\"UPDATE nagg_newsitemcollectionmembership SET data = %s WHERE id = %s;\"\"\",\n",
    "              (('{\"value\": \"%s\"}' % (_[1],), _[0]) for _ in iterator))\n",
    "    except Exception as ex:\n",
    "        conn.rollback()\n",
    "        yield False, 'partition {} rollback: {}'.format(idx, ex)\n",
    "    else:\n",
    "        conn.commit()\n",
    "        yield True, 'partition {} OK'.format(idx)\n",
    "#    yield str(conn)\n",
    "#    yield idx, [('{\"value\": \"%s\"}' % (_[1],), _[0]) for _ in iterator]\n",
    "\n",
    "res = rdd.mapPartitionsWithIndex(worker_update_results)\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this does not work with sqlcontext, only hivecontext\n",
    "qs = nic.items.filter(newsitemcollection__id=2)[:1000]\n",
    "qs.query.add_fields(['text', 'source', 'id'], True)\n",
    "df = app.data_from_from_sql(qs)\n",
    "writer = df.write.format('jdbc').options(\n",
    "            url='jdbc:postgresql://10.0.3.7/naggdb?user=nagg&password=nagg')\n",
    "writer.saveAsTable(app.sc.applicationId.replace('-', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "app.sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swedish 114\n",
      "hungarian 198\n",
      "portuguese 203\n",
      "finnish 229\n",
      "german 231\n",
      "norwegian 172\n",
      "turkish 53\n",
      "dutch 101\n",
      "spanish 313\n",
      "danish 94\n",
      "italian 279\n",
      "french 155\n",
      "english 127\n",
      "russian 151\n"
     ]
    }
   ],
   "source": [
    "stopwords = {}\n",
    "for lang in ['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian',\n",
    "             'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish',\n",
    "             'turkish']:\n",
    "    stopwords[lang] = set(nltk.corpus.stopwords.words(lang))\n",
    "\n",
    "for lang, words in stopwords.items():\n",
    "    print(lang, len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def guess_lang(text, n=2):\n",
    "    doc_words = set(nltk.word_tokenize(text))\n",
    "    scores = {}\n",
    "    for lang, sw_words in stopwords.items():\n",
    "        scores[lang] = len(sw_words.intersection(doc_words))\n",
    "    return [(l, s) for l, s in sorted(scores.items(), key=lambda x: -x[1])][0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(guess_lang('Hallo mijn naam is James'), 'dutch')\n",
    "print(guess_lang('Hello my name is James'), 'english')\n",
    "print(guess_lang('Hallo mein Name ist James'), 'german')\n",
    "print(guess_lang('Hola mi nombre es James'), 'spanish')\n",
    "print(guess_lang('Hej mitt namn r James'), 'swedish')\n",
    "print(guess_lang('Hei mitt navn er James'), 'norwegian')\n",
    "print(guess_lang('Hei, nimeni on James'), 'finish')\n",
    "print(guess_lang(\"Bonjour je m'appelle James\"), 'french')\n",
    "print(guess_lang('Hej mit navn er James'), 'danish')\n",
    "print(guess_lang(\",   \"), 'russian')\n",
    "print(guess_lang(\"Ciao il mio nome  James\"), 'italian')\n",
    "print(guess_lang(\"Hello a nevem James\"), 'hungarian')\n",
    "print(guess_lang(\"Ol meu nome  James\"), 'portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = {\n",
    "    'french': \"\"\"Un homme abattu et tu sa femme et deux autres  son domicile de Los Angeles le rveillon du Nouvel An avant que son fils a lutt le pistolet loin et fatalement lui a tir dans une chane d'vnements apparemment dclenches par un diffrend sur une machine  laver.\n",
    "\n",
    "Les deux autres victimes tues taient de 48 ans, ancienne petite amie de son fils qui vit aussi  la maison dans la rgion Rowland Heights de la ville et un homme de 27 ans qui tait en visite, le dpartement de thecounty shrif.\"\"\",\n",
    "    'english': \"\"\"A man shot and killed his wife and two others in his home in Los Angeles on New Years Eve before his son wrestled the gun away and fatally shot him in a chain of events apparently set off by a dispute over a washing machine. \n",
    "\n",
    "The two other victims killed were the sons 48-year-old girlfriend who also lives at the house in the Rowland Heights area of the city and a 27-year-old man who was visiting, thecounty sheriffs department said.\"\"\",\n",
    "    'dutch': \"\"\"Een man schoot en doodde zijn vrouw en twee anderen in zijn huis in Los Angeles op oudejaarsavond voor zijn zoon worstelde het pistool weg en doodgeschoten hem in een keten van gebeurtenissen blijkbaar gecompenseerd door een geschil over een wasmachine.\n",
    "\n",
    "De twee andere slachtoffers waren gedood, de zoon van de 48-jarige vriendin die ook woont in het huis in het Rowland Heights gebied van de stad en een 27-jarige man die op bezoek was, afdeling thecounty sheriff gezegd.\n",
    "\"\"\",\n",
    "    'german': \"\"\"Ein Mann erschoss seine Frau und zwei andere in seinem Haus in Los Angeles am Silvesterabend, bevor sein Sohn rangen die Waffe weg und erschossen ihn in eine Kette von Ereignissen offenbar durch einen Streit ber eine Waschmaschine setzen.\n",
    "\n",
    "Die beiden anderen Opfer gettet wurden, waren des Sohnes 48-jhrige Freundin, die auch lebt im Haus in der Rowland Heights Bereich der Stadt und einem 27-jhrigen Mann, der zu Besuch war, sagte Abteilungs thecounty Sheriffs.\"\"\",\n",
    "    'spanish': \"\"\"Un hombre dispar y mat a su esposa y otros dos en su casa de Los Angeles en la vspera de Ao Nuevo antes de que su hijo luch el arma y le dispar fatalmente en una cadena de acontecimientos aparentemente parti por una disputa sobre una lavadora.\n",
    "\n",
    "Las otras dos vctimas asesinadas eran 48 aos de edad, la novia del hijo, que tambin vive en la casa en la zona de Rowland Heights de la ciudad y un hombre de 27 aos de edad, que estaba de visita, dijo el departamento del sheriff thecounty.\"\"\",\n",
    "    'swedish': \"\"\"En man skt och ddade hans fru och tv andra i sitt hem i Los Angeles p nyrsafton innan hans son brottades pistolen bort och ddligt skt honom i en kedja av hndelser tydligen ivg av en tvist om en tvttmaskin.\n",
    "\n",
    "De tv andra offer ddade var sonens 48-riga flickvn som ocks bor i huset i omrdet Rowland Heights i staden och en 27-rig man som beskte, sade thecounty sheriff avdelning.\"\"\",\n",
    "    'norwegian': \"\"\"En mann skjt og drepte sin kone og to andre i sitt hjem i Los Angeles p nyttrsaften fr snnen kjempet pistolen bort og skjebnesvangert skjt ham i en kjede av hendelser tilsynelatende utlst av en krangel om en vaskemaskin.\n",
    "\n",
    "De to andre ofrene som ble drept var snnens 48 r gamle kjreste som ogs bor i huset i Rowland Heights-omrdet av byen og en 27-r gammel mann som var p besk, sa thecounty sheriff avdeling.\"\"\",\n",
    "    'danish': \"\"\"En mand skd og drbte sin kone og to andre i sit hjem i Los Angeles den nytrsaften, fr hans sn kmpede pistolen vk og ddeligt skudt ham i en kde af begivenheder tilsyneladende sat ud af en tvist over en vaskemaskine.\n",
    "\n",
    "De to andre ofre drbte var snnens 48-rige kreste, der ogs bor i huset i Rowland Heights omrde af byen og en 27-rig mand, der var p besg, sagde thecounty sheriffens afdeling.\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for lang in texts:\n",
    "    guess = guess_lang(texts[lang])\n",
    "    print(int(guess[0][0]==lang) , lang, guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nic.items.filter(newsitemcollection__id=1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-467adec87bc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'conn' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}